{
    "training_config": {
        "model_id": "meta-llama/Llama-2-7b-hf",
        "segment_len": 500,
        "total_batch_size": 32,
        "batch_size_per_device": 1,
        "device_count": 8,
        "gradient_accumulation_steps": 4,
        "learning_rate": 0.0001,
        "max_grad_norm": 1.0,
        "log_step": 10,
        "valid_step": 1000,
        "save_step": 1000000
    },
    "task_config": {
        "segment_size": 500,
        "mem_size": 100,
        "head_num": 5,
        "segment_num": 2,
        "task_type": "Compress"
    },
    "data_config": {
        "dataset_repo": "DKYoon/SlimPajama-6B",
        "hf_token": "123",
        "token_num": 1000000000,
        "min_len": 500,
        "model_id": "meta-llama/Llama-2-7b-hf"
    },
    "Total_parameters": 6888701952,
    "Trainable_parameters": 150552576,
    "Embedding_parameters": 5197824,
    "non-Embedding_parameters": 145354752
}