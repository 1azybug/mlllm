Dear Area Chair,

We hope this message finds you well. We would like to express our gratitude for your diligent efforts in managing the review process for our submission.

We are writing to address a specific concern raised by Reviewer 6s92, who commented on the clarity, logical coherence, and grammar of our manuscript. While we acknowledge that these aspects can indeed be improved in future revisions, we believe that this should not be the sole basis for rejection.

The content and contributions of our paper remain comprehensible and valuable to the field of long-context modeling, as evidenced by the constructive and positive feedback from Reviewers kxyu and EmpG. Both reviewers were able to grasp our work and provided valuable comments.

We appreciate your attention to this matter and are committed to enhancing the manuscript's clarity and coherence in the next version.

Best wishes,

All Authors


Main Rebuttal

We thank the reviewers for their constructive comments, and respond briefly to the main comments and criticisms.

## Response to EmpG

#### Response to W1:
No, the forgetting curve can also be applied to commercial LLMs. But using autoregressive APIs consumes more computational resources, we refrain from plotting the results for commercial LLMs out of environmental considerations. We will provide a detailed explanation after responding to S1.

#### Response to S1:
We do not use prompts to guide the model in completing the copying task. As shown in Figure 2, we fed the entire sequence, `target sequence + target sequence`, to the LLM in one go to obtain the logits for the all next tokens, and then used argmax to predict that tokens. We used the word **“teacher forcing”** to differentiate from the **autoregressive** generation method, which ensures that even if the LLM generates an incorrect token, the input tokens are always correct, meaning we do not use the tokens generated by the LLM as input. We did not use any prompts to inform the LLM that this is a copying task to avoid the influence of the prompt. You might wonder why the LLMs can complete the task without a prompt; as shown in Figure 2, we only calculate the accuracy of the tokens in the second half of the target sentence, and the first half of the tokens can be considered as a few-shot prompt. 

#### Response to W1:
The reason we did not investigate the forgetting curve on commercial LLMs is that these models only offer autoregressive generation. Although for the same sequence, we could make multiple requests to commercial LLMs to ensure that the input for each generated token is correct, such approach is not environmentally friendly or low-carbon, particularly when dealing with long sequence test scenarios.

#### Response to S2:
Yes, we believe that when the sequence is short, due to the insufficient number of tokens used for few-shot learning, LLMs are unable to comprehend that this is a copying task. Therefore, the accuracy is higher for longer sequences. In fact, the shortest sequence length we tested was 8, not 1. For example, when we see “121_”, it is difficult to realize that the complete sequence is “1212”; however, if we see “123456781234_ _ _ _”, we are more likely to infer that the complete sequence is “1234567812345678”.

#### Response to S3:
We chose to use the PG-19 dataset because it is sufficiently long and consists of real English text, which aligns with the distribution of most LLMs. As shown in Figure 3(b), we also used Chinese text and randomly generated token sequences, which exhibit similar lengths before entering the amnesia phase. Moreover, the forgetting curves shown by both the Chinese and PG-19 datasets lead to similar conclusions. We believe that the forgetting curve demonstrates a certain degree of robustness across different data sources. More discussions can be found in Section 3.3.

#### Response to S4:
Yes, as mentioned in line 112, we will release our code and data. We will ensure it is user-friendly. After cloning our repository, anyone can use the following command to evaluate open-source LLMs on Hugging Face that support teacher forcing input (which is essentially the forward function of LLMs):
```
python draw.py 
    --model_id meta-llama/Meta-Llama-3-8B \
    --title Meta-Llama-3-8B \
    --repeat_time 10 \
    --granularity 32 \
    --test_max_length 16000 \
    --training_len 8000 \
```
Additionally, we have provided an interface for drawing the forgetting curve for custom models:
```
from draw import evaluate
evaluate(model, tokenizer, texts_or_ids=test_tokens, config=config)
```

## Response to kxyu

#### Response to W1:
Yes, the copy task can be used for instruction-tuned models. There are 5 instruction-tuned models among the 14 LLMs we tested, and the results are shown in Appendix C. As illustrated in Figure 13, Mistral-7B-Instruct-v0.2 can maintain a 100% accuracy rate over a wide range for the copy task. In the experimental setup of the forgetting curve, the accuracy of the copy task, and also language model task, was measured using teacher forcing (see figure 2, line 243). Unlike the autoregressive generation method, teacher forcing ensures that the model makes predictions based on the correct preceding text at each position, rather than relying on potentially incorrect tokens generated by the LLM. That’s one main reason why we could draw the copy accuracy curve for instruction-tuned models.



#### Response to W2:
No, `irrelevant text + target sequence` only tests the LLMs’ **generation capability** for the target sequence. The language modelling task itself does not test long-context capacity; it primarily serves as a reference for the copy task (line 256). One should notice that even without any information related to the target sequence, LLMs can still generate the target sequence. The generation capability is a fundamental ability of LLMs, and knowledge about the target sequence can be extracted from the parameters of LLMs. In copy task, `target sequence + target sequence` is influenced by both the **generation capability** and **memory capacity** of LLMs. By **comparing** the accuracy of copy and lm task, we can understand the impact of LLMs' memory capacity. However, `target sequence + suffix` cannot provide a comparable result.

## Response to 6s92

Thank you for your feedback and suggestions. We acknowledge the grammatical errors and the need for improved logical flow and coherence in the manuscript. We will thoroughly revise the entire manuscript to address these issues. While we recognize that the current version requires further improvement, we believe that the paper's content and contributions are still comprehensible, and may contribute to the long-context modeling area in the literature. We would greatly appreciate more detailed feedback on the technical contributions and other substantive aspects of our work. On the other hand, though you mentioned that our paper is difficult to understand, reviewers kxyu and EmpG were able to grasp our work and provided valuable comments. 

Please allow us to briefly introduce our work:

We found that existing long-context evaluation methods have many limitations, such as "Needle in a Haystack" prompts that can severely affect a model's score, and models with small memory capacity can easily hack them, etc. We first defined and categorized the limitations of existing evaluation methods. Against this background, **the long-context LLM community needs a more reliable evaluation method.** Therefore, we proposed the forgetting curve to test the memory capability of LLMs and conducted experiments on it. The experimental results show that the forgetting curve has little or none of the limitations mentioned in Section 2. It is a simple, easy-to-use, and reliable evaluation method. We further depicted the forgetting curves of 14 open-source LLMs, and provided some interesting analyses.

Our contributions are as follows:

* We survey the existing long-sequence evaluation metrics, identifying five unique limitations and highlighting their issues.
* We propose the forgetting curve, a method that visualizes the memory length of any given language model.
* We evaluate long-context language models using the forgetting curve and perform a cross-model analysis.
* Our method decouples long-term understanding and memorization capability, providing a new perspective for analyzing and enhancing models' long-sequence modeling abilities.
* We will release our code and data to the community for evaluating new models that emerge in the future.

We hope to receive further comments from you and would be very grateful.






